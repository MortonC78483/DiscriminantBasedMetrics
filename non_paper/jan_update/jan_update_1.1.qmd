---
title: "Jan_Update_1.1"
format:
  html:
    self-contained: true
    theme: default
    code-fold: true
    code-summary: "Show the code"
    embed-resoures: true
    toc: true
---

```{r}
library(tidyverse)
library(ipumsr)
library(srvyr)
library(tidysynthesis)
library(parsnip)
library("data.table")
library(recipes)
library(devtools)
load_all('../../syntheval')
library(syntheval)
library(dials)
library(tune)
library(gtools)
library(MASS)
library(caret)
library(gt)
library(ggpubr)

set.seed(1)
```

This piece of the January Update concerns the first part of the project.

In this project, I'm trying to find ways to detect when synthesized data isn't just generally a poor match for original confidential data, but when it specifically matches confidential data less for a minority group.

The use of synthetic datasets, or datasets that are produced to imitate confidential datasets, are a way to inform evidence-based policy while protecting privacy. Synthetic data are especially important because, in a given dataset, individuals belonging to minority groups are more identifiable than individuals belonging to majority groups. By creating new people in a synthetic dataset, we can preserve the privacy of the true members of the confidential dataset. However, in order to be useful, synthetic data must be accurate -- analyses performed with synthetic data cannot have different conclusions to the confidential data, because this means the synthetic data are uninformative (or worse, misleading) for policy decisions. 

This balance between privacy and utility is critical to produce useful synthetic data, and it is especially hard to strike for minority groups in the data. If the synthetic data disguise the characteristics of minority individuals in the confidential data more than majority individuals, approaching equal privacy protections for majority and minority groups, they have less utility for members of minority groups. If utility is prioritized, then members of minority groups are more identifiable and thus have less of their privacy preserved compared to their majority group counterparts. The question of balancing privacy and utility should be settled on a dataset-by-dataset basis and in collaboration with members of the minority groups who may risk privacy violations at the cost of accurate analysis and evidence-based policy. However, we cannot begin conversations around this question without first having accurate and well-understood approaches to compare the utility of a given synthetic dataset across majority and minority groups. This is the task I'm working on in this project using discriminant-based metrics.

Discriminant-based metrics measure how well a model is able to discriminate between the synthetic and confidential datasets with the idea that, if the datasets are hard for the model to tell apart, the synthetic data are a good match to the confidential data.

One issue with discriminant-based metrics is that a model that does a poor job at telling apart synthetic and confidential data may do poorly because the data are actually similar but may also do poorly because it is not a good model. In order to determine which kinds of models I should use in the rest of this project (to make it more likely that a model that could not discriminate between the datasets was identifying that synthetic data were a good match to confidential data), I did some preliminary analyses. 

In these analyses, I'm creating issues in the synthetic data (specifically: changing variable means, changing variable correlations, and creating a dataset that is off on both means and variances) and testing which types of models are best able to detect these issues. I tested three different metrics: SPECKS, pMSE, and AUC. The model types I'm testing are: a classification tree (tree), a logistic regression (lr), a lasso regression (lasso), and a random forest (rf).

```{r}
n=10000 # number of samples
corr=.7 # correlation
p = 10 # number of variables

generate_postsynth <- function(data){
  list(
    synthetic_data = data,
    jth_synthesis_time = data.frame(
      variable = factor(colnames(data.frame(data)))
    )
  )  %>%
  structure(class = "postsynth")
}
```

```{r}
# Same distribution
vcov = matrix(corr, p, p) + diag(1-corr, p, p) # vcov matrix, p on all off-diagonal variables
means = rep(0,p) # mean vector
conf1 = as_tibble(mvrnorm(means, vcov, n = n), n = p)
synth1 = generate_postsynth(as_tibble(mvrnorm(means, vcov, n = n), n=p))

# larger variances, slightly off means
vcov = matrix(corr, p, p) + diag(1-corr, p, p) # vcov matrix, p on all off-diagonal variables
means = rep(0,p) # mean vector
conf2 = as_tibble(mvrnorm(means, vcov, n = n), n=p)
means = sample(c(-.2,.2), 10, replace = T) # mean vector
vcov = matrix(2*corr, p, p) + diag(4-2*corr, p, p) # vcov matrix, p on all off-diagonal variables
synth2 = generate_postsynth(as_tibble(mvrnorm(means, vcov, n = n), n=p))

# one variable centered at wrong value
vcov = matrix(corr, p, p) + diag(1-corr, p, p) # vcov matrix, p on all off-diagonal variables
means = rep(0,p) # mean vector
conf3 = as_tibble(mvrnorm(means, vcov, n = n), n=p)
means[1] = 2
synth3 = generate_postsynth(as_tibble(mvrnorm(means, vcov, n = n), n=p))

# relationship reversed (between V1 and everything else)
vcov = matrix(corr, p, p) + diag(1-corr, p, p) # vcov matrix, p on all off-diagonal variables
means = rep(0,p) # mean vector
conf4 = as_tibble(mvrnorm(means, vcov, n = n), n=p)
vcov[1,2:p] = -rep(corr, p-1)
vcov[2:p,1] = -rep(corr, p-1)
synth4 = generate_postsynth(as_tibble(mvrnorm(means, vcov, n = n), n=p))
```

```{r}
# create models
tree_mod <- decision_tree(cost_complexity = tune()) %>%
  set_mode(mode = "classification") %>%
  set_engine(engine = "rpart")

lr_mod <- logistic_reg(engine = "glm") %>%
  set_mode(mode = "classification")

lasso_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%
    set_mode(mode = "classification") %>%
    set_engine(engine = "glmnet")  

rf_mod <- rand_forest(
    mode = "classification",
    engine = "randomForest",
    mtry = NULL,
    trees = NULL,
    min_n = NULL
)
```

```{r}
# function to return the discriminator object with propensity scores for synthetic and confidential data
# and a model
# provide synthetic data, confidential data, model, name of model used (type), name of data (name), and parameter to tune, if any
make_disc <- function(synth, conf, mod, type, name, param = "none") {
  rpart_rec <- recipe(.source_label ~ ., data = discrimination(synth, conf)$combined_data)
  
  if (param == "none"){
    # set up discriminator
    d <- discrimination(synth, conf) %>%
      add_propensities(
        recipe = rpart_rec,
        spec = mod
      ) %>%
      add_discriminator_auc() %>%
      add_specks() %>%
      add_pmse()
  }
  else{
    if (param == "penalty"){ # lasso model
      rec = rpart_rec %>%
        step_poly(all_numeric_predictors(), degree = 2) %>%
        step_normalize(all_predictors())
      grid = grid_regular(penalty(), levels = 10)
    }
    else if (param == "cp"){ # tree model
      rec = rpart_rec 
      grid = grid_regular(cost_complexity(), levels= 10)
    }
    # set up discriminator
    d <- discrimination(synth, conf) %>%
      add_propensities_tuned(
        grid = grid, 
        recipe = rec,
        spec = mod
      ) %>%
      add_discriminator_auc() %>%
      add_specks() %>%
      add_pmse()
  }
  # extract the metrics
  c(d$pmse$.pmse[2], d$specks$.specks[2], d$discriminator_auc$.estimate[2], 
    type, name)
}
```

```{r}
# iterate through all of the model pairs we've got
pairs <- list(list(synth1, conf1, "Control"),
           list(synth2, conf2, "All"),
           list(synth3, conf3, "Mean"),
           list(synth4, conf4, "Correlation"))

df = data.frame(NULL)

for (elem in pairs) {
  # for each type of pairing, we need to evaluate both model types on cont and cat data
  df = rbind(df, make_disc(elem[[1]], elem[[2]], lr_mod, "lr", elem[[3]], "none"))
  df = rbind(df, make_disc(elem[[1]], elem[[2]], tree_mod, "tree", elem[[3]], "cp"))
  df = rbind(df, make_disc(elem[[1]], elem[[2]], lasso_mod, "lasso", elem[[3]], "penalty"))
  df = rbind(df, make_disc(elem[[1]], elem[[2]], rf_mod, "rf", elem[[3]], "none"))
}

colnames(df) = c("pmse", "specks", "auc", "type", "name")

# indicates control (synthetic and confidential data from same distribution), errors with both mean and variance, 
# errors with correlations, and errors with means
level_order <- c("Control", "All", "Correlation", "Mean") 
group_colors <- c(lr ="#361052", tree = "green", lasso = "red", rf = "blue")
```

```{r}
fig_a = ggplot(df, aes(x = factor(name, level = level_order), 
                       group = type, 
                       color = type, 
                       y = as.numeric(specks)))+
  geom_jitter(width = .03, size = 4)+
  ylab("SPECKS Value")+
  xlab("")+
  ggtitle("SPECKS")+
  scale_color_manual(values=group_colors)+
  theme_classic()

fig_b = ggplot(df, aes(x = factor(name, level = level_order), group = type, 
                       color = type, y = as.numeric(pmse)))+
  geom_jitter(width = .03, size = 4)+
  ylab("pMSE Value")+
  xlab("")+
  ggtitle("pMSE")+
  scale_color_manual(values=group_colors)+
  theme_classic()

fig_c = ggplot(df, aes(x = factor(name, level = level_order), group = type, 
                       color = type, y = as.numeric(auc)))+
  geom_jitter(width = .03, size = 4)+
  ylab("AUC Value")+
  xlab("")+
  ggtitle("AUC")+
  scale_color_manual(values=group_colors)+
  theme_classic()

ggarrange(fig_a + scale_x_discrete(guide = guide_axis(n.dodge = 2)), 
          fig_b + scale_x_discrete(guide = guide_axis(n.dodge = 2)),
          fig_c + scale_x_discrete(guide = guide_axis(n.dodge = 2)))
```

From this figure, we can see that generally the random forest model and the tree model did the best at discriminating between synthetic and confidential datasets when there were errors in the synthetic data.

