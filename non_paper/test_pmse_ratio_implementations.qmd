---
title: "test_pmse_ratio_implementations"
format: pdf
---

This document is going to test what happens if we use different implementations of the pMSE ratio for how the models are trained in the denominator.

2 model approach:
Class 1: No grouping of pMSE ratio denominators (no adjustment)
1) Each denominator component is calculated by completely refitting the existing model, including hyperparameter retuning
2) Each denominator component is calculated by refitting the existing model but keeping the chosen hyperparameters, so the trees in the numerator and denominator models have to have the same characteristics.
1.1 is what pMSE ratio is currently doing when you have groups (in my version of syntheval).

Class 2: Grouping of pMSE ratio denominators (denominator adjustment)
1 and 2 are same.

I will then look at the impact that this has on the group metric values for the "bad" synthesis only.

Load libraries
```{r}
library(tidyverse)
library(ipumsr)
library(srvyr)
library(tidysynthesis)
library(parsnip)
library("data.table")
library(recipes)
library(devtools)
load_all("../syntheval")
#library(syntheval)
library(dials)
library(tune)
library(gtools)
library(MASS)
library(caret)
library(gt)
library(ggpubr)
source("add_pmse_ratio_size.R")
library(rlang)
set.seed(78483)
```

Load data
```{r}
process_data <- function(df){
  df <- as.data.frame(unclass(df),stringsAsFactors=TRUE)
  df$pov <- as.factor(df$pov)
  df$BLACK <- as.factor(df$BLACK)
  order = c("BLACK", "pov", "SEX", "EMPSTAT", "EDUC", "HCOVANY", "VETSTAT", "FTOTINC", "AGE")
  df <- df %>%
    dplyr::select(order)
  df
}

data_mi <- read_csv("data/data_mi.csv")
data_mi <- process_data(data_mi)

synth_bad <- read_csv("data/bad_synth_black.csv")
synth_bad <- process_data(synth_bad)
```

Fit discriminator
```{r}
calc_disc <- function(discriminator){
  # Evaluate discriminant-based metrics on the data using tree-based model
  tree_mod <- decision_tree(cost_complexity = tune()) %>%
    set_mode(mode = "classification") %>%
    set_engine(engine = "rpart")
  
  rpart_rec <- recipe(.source_label ~ ., data = discriminator$combined_data)
  
  grid = grid_regular(cost_complexity(), levels= 10)
  
  # set up discriminator
  d <- discriminator %>%
    add_propensities_tuned(
      grid = grid, 
      recipe = rpart_rec,
      spec = tree_mod
    )
  d
}

disc_bad = discrimination(synth_bad, data_mi)
res_bad = calc_disc(disc_bad)

conf_not_black <- data_mi %>%
    filter(BLACK == 0)
synth_not_black <- synth_bad %>%
    filter(BLACK == 0)
disc_not_black <- discrimination(synth_not_black, conf_not_black)
  # fit model wtih pmse to our not Black sub-dataset
res_not_black <- calc_disc(disc_not_black)
cp_not_black = quo_get_expr(extract_spec_parsnip(res_bad$discriminator)$args$cost_complexity)

# create Black sub-dataset
conf_black <- data_mi %>%
    filter(BLACK == 1)
synth_black <- synth_bad %>%
    filter(BLACK == 1)
disc_black <- discrimination(synth_black, conf_black)

# fit model wtih pmse to Black sub-dataset
res_black <- calc_disc(disc_black)
cp_black = quo_get_expr(extract_spec_parsnip(res_bad$discriminator)$args$cost_complexity)

c(cp_not_black, cp_black)
```

Calculate pMSE ratio for one example denominator for minority and majority, and fetch the parameters to see if they're different
```{r}
prop = 3/4
null_pmse_no_refit <- function(data){
  calc_pmse <- function(propensities) {
      # calculate the expected propensity
      prop_synthetic <- propensities %>%
        dplyr::summarize("prop_synthetic" = list(c(sum(.data$.source_label == "synthetic")/dplyr::n()))) %>%
        dplyr::pull(prop_synthetic)
      
      propensities_vec <- propensities %>%
        dplyr::summarise(".pred_synthetic" = list(c(.pred_synthetic))) %>%
        dplyr::pull(.pred_synthetic)
      
      # function for pmse
      pmse_func <- function(propensities_vec, prop_synthetic){
        mean((propensities_vec - prop_synthetic)^2)
      }
      
      # calculate the observed pMSE
      #pmse <- mapply(pmse_func, propensities_vec, prop_synthetic)
      pmse <- purrr::map2_dbl(
        .x = propensities_vec,
        .y = prop_synthetic,
        .f = pmse_func
      )
      return(pmse)
  }
  
  resample <- function(data){
    bootstrap_sample <- list(dplyr::bind_cols(
      data %>%
        dplyr::filter(.data$.source_label == "original") %>%
        dplyr::slice_sample(n = nrow(data), replace = TRUE) %>%
        dplyr::select(-".source_label"),
      data %>%
        dplyr::select(".source_label")))
      
      bootstrap_sample = dplyr::bind_rows(bootstrap_sample)
  } 
  
  pmse_null_overall <- vector(mode = "numeric", length = 1)
  pmse_null_training <- vector(mode = "numeric", length = 1)
  pmse_null_testing <- vector(mode = "numeric", length = 1)
    
  # bootstrap sample original observations to equal the size of the combined 
  # data, with a vector of one set of observations per grouping variable (?)
  # append the original labels so the proportions match
  bootstrap_sample <- resample(data$combined_data)
      
  # make training/testing split 
  data_split <- rsample::initial_split(
      data = bootstrap_sample,
      prop = prop,
      strata = ".source_label" # NOTE: Should this also be stratified by group?
  )
        
  # fit the model from the pMSE on the bootstrap sample
  fitted_model <- parsnip::fit(
    data$discriminator, 
    data = rsample::training(data_split)
  )
  
  print(paste0("cp: ", quo_get_expr(extract_spec_parsnip(fitted_model)$args$cost_complexity)))
       
  # calculate the propensities
  propensities_df <- dplyr::bind_cols(
    stats::predict(fitted_model, new_data = data$combined_data, type = "prob")[, ".pred_synthetic"],
    data$combined_data
  ) %>%
      dplyr::mutate(
        .sample = dplyr::if_else(
          dplyr::row_number() %in% data_split$in_id, 
          true = "training", 
          false = "testing"
        )
          )
        
  pmse_null_testing[1] <- list(propensities_df %>%
                                dplyr::filter(.data$.sample == "testing") %>%
                                calc_pmse())
     
  # find the mean of the bootstrapped pMSEs
  mean_null_pmse_testing <- colMeans(do.call(rbind,pmse_null_testing))
    
  mean_null_pmse_testing
}

null_pmse_black = null_pmse_no_refit(res_black)
print(paste0("null pmse black: ", null_pmse_black))
null_pmse_not_black = null_pmse_no_refit(res_not_black)
print(paste0("null pmse not black: ", null_pmse_not_black))
```

Calculate pMSE ratios
```{r}
res_black <- res_black %>%
  add_pmse()
print("ratio black")
res_black$pmse$.pmse[2]/null_pmse_black

res_not_black <- res_not_black %>%
  add_pmse()
print("ratio not black")
res_not_black$pmse$.pmse[2]/null_pmse_not_black
```

I observed the two parameters were the same because parsnip::fit doesn't change them at all, just takes the discriminator model from the numerator. So I refit and got:
```{r}
prop = 3/4
null_pmse_refit <- function(data){
  calc_pmse <- function(propensities) {
      
      # calculate the expected propensity
      prop_synthetic <- propensities %>%
        dplyr::summarize("prop_synthetic" = list(c(sum(.data$.source_label == "synthetic")/dplyr::n()))) %>%
        dplyr::pull(prop_synthetic)
      
      propensities_vec <- propensities %>%
        dplyr::summarise(".pred_synthetic" = list(c(.pred_synthetic))) %>%
        dplyr::pull(.pred_synthetic)
      
      # function for pmse
      pmse_func <- function(propensities_vec, prop_synthetic){
        mean((propensities_vec - prop_synthetic)^2)
      }
      
      # calculate the observed pMSE
      #pmse <- mapply(pmse_func, propensities_vec, prop_synthetic)
      pmse <- purrr::map2_dbl(
        .x = propensities_vec,
        .y = prop_synthetic,
        .f = pmse_func
      )
      return(pmse)
  }
  
  resample <- function(data){
    bootstrap_sample <- list(dplyr::bind_cols(
      data %>%
        dplyr::filter(.data$.source_label == "original") %>%
        dplyr::slice_sample(n = nrow(data), replace = TRUE) %>%
        dplyr::select(-".source_label"),
      data %>%
        dplyr::select(".source_label")))
      
      bootstrap_sample = dplyr::bind_rows(bootstrap_sample)
  } 
  
  pmse_null_overall <- vector(mode = "numeric", length = 1)
  pmse_null_training <- vector(mode = "numeric", length = 1)
  pmse_null_testing <- vector(mode = "numeric", length = 1)
    
  # bootstrap sample original observations to equal the size of the combined 
  # data, with a vector of one set of observations per grouping variable (?)
  # append the original labels so the proportions match
  bootstrap_sample <- resample(data$combined_data)
      
  # make training/testing split 
  data_split <- rsample::initial_split(
      data = bootstrap_sample,
      prop = prop,
      strata = ".source_label" # NOTE: Should this also be stratified by group?
  )
  
  # fit model, retraining cp
  training = rsample::training(data_split)
  training_synth = training[training$.source_label == "synthetic",]
  training_conf = training[training$.source_label == "original",]
  
  disc_sample <- calc_disc(discrimination(training_synth, training_conf))
  fitted_model = disc_sample$discriminator
  
  print(paste0("cp: ", quo_get_expr(extract_spec_parsnip(fitted_model)$args$cost_complexity)))

  # calculate the propensities
  propensities_df <- dplyr::bind_cols(
    stats::predict(fitted_model, new_data = data$combined_data, type = "prob")[, ".pred_synthetic"],
    data$combined_data
  ) %>%
      dplyr::mutate(
        .sample = dplyr::if_else(
          dplyr::row_number() %in% data_split$in_id, 
          true = "training", 
          false = "testing"
        )
          )
        
  pmse_null_testing[1] <- list(propensities_df %>%
                                dplyr::filter(.data$.sample == "testing") %>%
                                calc_pmse())
     
  # find the mean of the bootstrapped pMSEs
  mean_null_pmse_testing <- colMeans(do.call(rbind,pmse_null_testing))
    
  mean_null_pmse_testing
}

null_pmse_black <- null_pmse_refit(res_black)  
print(paste0("null pmse black: ", null_pmse_black))
null_pmse_not_black <- null_pmse_refit(res_not_black)
print(paste0("null pmse not black: ", null_pmse_not_black))
```

Note that once when I ran this code, it picked cp large and got null pMSE 0. Think we shouldn't allow this (maybe cp has to be at least as large as the numerator??)


Calculate pMSE ratios (with retuning)
```{r}
res_black <- res_black %>%
  add_pmse()
print("ratio black")
res_black$pmse$.pmse[2]/null_pmse_black

res_not_black <- res_not_black %>%
  add_pmse()
print("ratio not black")
res_not_black$pmse$.pmse[2]/null_pmse_not_black
```

Code to do the size correction (no refit)
```{r}
null_pmse_size_correct <- function(data, size){
  calc_pmse <- function(propensities) {
      
      # calculate the expected propensity
      prop_synthetic <- propensities %>%
        dplyr::summarize("prop_synthetic" = list(c(sum(.data$.source_label == "synthetic")/dplyr::n()))) %>%
        dplyr::pull(prop_synthetic)
      
      propensities_vec <- propensities %>%
        dplyr::summarise(".pred_synthetic" = list(c(.pred_synthetic))) %>%
        dplyr::pull(.pred_synthetic)
      
      # function for pmse
      pmse_func <- function(propensities_vec, prop_synthetic){
        mean((propensities_vec - prop_synthetic)^2)
      }
      
      # calculate the observed pMSE
      #pmse <- mapply(pmse_func, propensities_vec, prop_synthetic)
      pmse <- purrr::map2_dbl(
        .x = propensities_vec,
        .y = prop_synthetic,
        .f = pmse_func
      )
      return(pmse)
  }
  
  resample <- function(data, size){
    combined_sample <- data %>%
        dplyr::filter(.data$.source_label == "original") %>%
        sample_n(size, replace = FALSE)
      
    bootstrap_sample <- combined_sample %>%
          dplyr::slice_sample(n = 2*size, replace = TRUE) %>%
          dplyr::select(-".source_label")
    bootstrap_sample$.source_label = c(rep("original", size), rep("synthetic", size))
    return(bootstrap_sample)
  } 
  
  pmse_null_overall <- vector(mode = "numeric", length = 1)
  pmse_null_training <- vector(mode = "numeric", length = 1)
  pmse_null_testing <- vector(mode = "numeric", length = 1)
    
  # bootstrap sample original observations to equal the size of the combined 
  # data, with a vector of one set of observations per grouping variable (?)
  # append the original labels so the proportions match
  bootstrap_sample <- resample(data$combined_data, size)
      
  # make training/testing split 
  data_split <- rsample::initial_split(
      data = bootstrap_sample,
      prop = prop,
      strata = ".source_label" # NOTE: Should this also be stratified by group?
  )
  
  # fit model, retraining cp
  fitted_model <- parsnip::fit(
    data$discriminator, 
    data = rsample::training(data_split)
  )
  
  print(paste0("cp: ", quo_get_expr(extract_spec_parsnip(fitted_model)$args$cost_complexity)))
  
  # calculate the propensities
  propensities_df <- dplyr::bind_cols(
    stats::predict(fitted_model, new_data = data$combined_data, type = "prob")[, ".pred_synthetic"],
    data$combined_data
  ) %>%
      dplyr::mutate(
        .sample = dplyr::if_else(
          dplyr::row_number() %in% data_split$in_id, 
          true = "training", 
          false = "testing"
        )
          )
        
  pmse_null_testing[1] <- list(propensities_df %>%
                                dplyr::filter(.data$.sample == "testing") %>%
                                calc_pmse())
     
  # find the mean of the bootstrapped pMSEs
  mean_null_pmse_testing <- colMeans(do.call(rbind,pmse_null_testing))
    
  mean_null_pmse_testing
}

```

```{r}
minority_size = nrow(res_black$combined_data)/2
null_pmse_black <- null_pmse_size_correct(res_black, minority_size) 
print(paste0("null pmse black", null_pmse_black))
null_pmse_not_black <- null_pmse_size_correct(res_not_black, minority_size)
print(paste0("null pmse not black", null_pmse_not_black))
```

```{r}
res_black <- res_black %>%
  add_pmse()
print("ratio black")
res_black$pmse$.pmse[2]/null_pmse_black

res_not_black <- res_not_black %>%
  add_pmse()
print("ratio not black")
res_not_black$pmse$.pmse[2]/null_pmse_not_black
```




