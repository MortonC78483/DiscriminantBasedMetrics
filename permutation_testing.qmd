---
title: "permutation_testing"
format:
  html:
    theme: default
---

I noticed in the original analysis I did for the poverty grouping that even in the version that is supposed to do a good job at synthesizing the data, working equally well on majority and minority groups, there still seems to be a bias towards saying it's easier to tell apart the below poverty line folks compared to the above poverty line folks. I wanted to try doing some random divisions of the data into groups of the same size to compare the discriminant-based metric values and see whether (for this dataset and these group sizes) we always expect the models to do a better job at telling apart small datasets (and, if so, how much). 

This is to test whether this type of method could be a useful way to tell if your synthesis is biased, since we do get variation in metrics which can be large(ish) because of the cross-validation.

I then would want to do a similar comparison for the synthesis just on people above the poverty line.

```{r, echo = F}
library(tidyverse)
library(ipumsr)
library(srvyr)
library(parsnip)
library("data.table")
library(recipes)
library(devtools)

load_all('../tidysynthesis')
library(tidysynthesis)
load_all('../syntheval')
library(syntheval)
library(dials)
library(tune)
library(gtools)
library(MASS)
library(caret)
library(gt)
library(ggpubr)

set.seed(1)
```

```{r}
# pull dataset
data <- read_ipums_micro("data/usa_00004.xml")
data <- data |>
  mutate(across(where(is.labelled), ~as_factor(lbl_clean(.x))))

states <- c("Michigan")
vars <- c("STATEFIP", "PERNUM", "SEX", "AGE", "RACE", 
           "HCOVANY", "EDUC", "EMPSTAT", "FTOTINC", "POVERTY",
           "VETSTAT")

data_mi <- data %>%
  dplyr::select(all_of(vars)) %>%
  filter(STATEFIP %in% states,
         PERNUM == 1) 

# recode factor variables (vetstat, sex, race, healthcare coverage, employment status didn't need recoding)
data_mi$EDUC <- recode(data_mi$EDUC, 
               "N/A or no schooling" = "N/A",
               "Nursery school to grade 4" = "Less than high school",
               "Grade 5, 6, 7, or 8" = "Less than high school",
               "Grade 9" = "Some high school",
               "Grade 10" = "Some high school",
               "Grade 11" = "Some high school",
               "Grade 12" = "High school",
               "1 year of college" = "Some college",
               "2 years of college" = "Some college",
               "4 years of college" = "4+ years of college",
               "5+ years of college" = "4+ years of college")

# recode numeric variables (age, poverty (as % of poverty threshold), and income)
data_mi$AGE <- as.numeric(as.character(recode(data_mi$AGE, 
                      "Less than 1 year old"= "0",
                      "90 (90+ in 1980 and 1990)" = "90")))
data_mi$FTOTINC <- as.numeric(as.character(recode(data_mi$FTOTINC,
                                     "No income (1950-2000, ACS/PRCS) " = "0")))
data_mi$POVERTY <- as.numeric(as.character(recode(data_mi$POVERTY,
                                     "501 percent or more of poverty threshold" = "500",
                                     "1 percent or less of poverty threshold (including 0 or negative income)" = "0")))

# set character NAs to be NA
data_mi[data_mi == "N/A"]<-NA 

# create poverty indicator
data_mi$pov <- ifelse(data_mi$POVERTY < 100, 1, 0)

# complete case analysis of adults only
data_mi <- data_mi %>%
  filter(AGE >= 19,
         complete.cases(.)) %>%
  dplyr::select(-c("STATEFIP", "PERNUM", "POVERTY"))

# get rid of unused NA levels in factors (educ, empstat, vetstat)
data_mi = droplevels(data_mi)
```

```{r}
# function that takes start data and confidential data, and synthesizes dataset
synth_pov <- function(start_data, conf_data){
  # synthesize categorical first, then continuous
  visit_sequence = visit_sequence(c("SEX", "VETSTAT", "EMPSTAT", "EDUC", "HCOVANY", "RACE", "FTOTINC", "AGE"), start_data, type = "manual")
  roadmap = roadmap(conf_data, start_data, visit_sequence)
  recipe = construct_recipes(roadmap = roadmap)
  
  tree_cl <- parsnip::decision_tree(cost_complexity = .0001) %>%
    set_mode(mode = "classification") %>%
    set_engine(engine = "rpart")
  tree_reg <- parsnip::decision_tree(cost_complexity = .0001) %>%
    set_mode(mode = "regression") %>%
    set_engine(engine = "rpart")
  
  synth_spec = synth_spec(roadmap,
                          synth_algorithms = list(tree_cl, tree_cl, tree_cl, tree_cl, tree_cl, tree_cl, tree_reg, tree_reg),
                          recipe,
                          predict_methods = sample_rpart)
  
  # noise
  noise <- noise(roadmap = roadmap,
                 add_noise = FALSE,
                 exclusions = 0)
  
  # constraints
  constraints <- constraints(roadmap = roadmap,
                             constraints = NULL,
                             max_z = 0)
  
  replicates <- replicates(replicates = 1,
                           workers = 1,
                           summary_function = NULL)
  
  # create a presynth object
  presynth1 <- presynth(
    roadmap = roadmap,
    synth_spec = synth_spec,
    noise = noise, 
    constraints = constraints,
    replicates = replicates
  )
  synthesized = synthesize(presynth1, progress = TRUE)
  synthesized
}
```

```{r}
# functions to apply tree model discriminant-based metrics
disc_tree <- function(discriminator){
  # Evaluate discriminant-based metrics on the data using tree-based model
  tree_mod <- decision_tree(cost_complexity = tune()) %>%
    set_mode(mode = "classification") %>%
    set_engine(engine = "rpart")
  
  rpart_rec <- recipe(.source_label ~ ., data = discriminator$combined_data)
  
  grid = grid_regular(cost_complexity(), levels= 10)
  
  # set up discriminator
  d <- discriminator %>%
    add_propensities_tuned(
      grid = grid, 
      recipe = rpart_rec,
      spec = tree_mod
    ) %>%
    add_discriminator_auc()
  res = c(d$discriminator_auc)
  return(res$.estimate[2])
}

```

I'm trying to tell whether, when we have majority/minority splits, if we always expect the models run on isolated majority/minority to have the bias of higher metric values for the minority group. To do this, I'm sampling rows from the synthesized and confidential datasets, then using these sampled rows as my minority datasets and the nonsampled rows as my majority datasets. This ensures both pairs of synthetic-confidential datasets have the same number of people in poverty. Then, I'm training new models on the datasets and calculating the auc for the tree model, as an example.

```{r}
# synthesize dataset from the full original dataset
start_data = data_mi[,"pov"]
conf_data = data_mi
synthesized <- synth_pov(start_data, conf_data)

# calculate the discriminant-based metrics (done previously)
N = 20
n_minority = sum(data_mi$pov == 1)

min_auc = c()
maj_auc = c()

# N times:
for (i in 1:N){
  # Select randomly n rows of synthetic/confidential datasets, where n = number of people in poverty
  sample = sample(x = 1:nrow(data_mi),
                  size = n_minority,
                  replace = FALSE)
  
  # Divide into four datasets -- synth min, synth maj, conf min, conf maj
  conf_min = data_mi[sample,]
  synth_min = synthesized$synthetic_data[sample,]
  conf_maj = data_mi[-sample,]
  synth_maj = synthesized$synthetic_data[-sample,]
  
  # Run discriminant-based metrics on synth min vs conf min, synth maj vs conf maj
  # Store the CART AUC values (just picking one metric to start) in two vectors, one for maj, one for min
  # isolate to minority group
  d_min = discrimination(synth_min, conf_min)
  d_tree_min = disc_tree(d_min)
  min_auc = append(min_auc, d_tree_min)
  
  # isolate to majority group
  d_maj = discrimination(synth_maj, conf_maj)
  d_tree_maj = disc_tree(d_maj)
  maj_auc = append(maj_auc, d_tree_maj)
}

min_auc-maj_auc
# Compare distributions (compute a p-value)
# observed min_auc was .645, maj_auc was .584
ecdf = ecdf(min_auc-maj_auc)
ecdf(.645-.584)
```

Our observed p-value is 0 (since 0 differences of min_auc-maj_auc were greater than the observed difference). I only ran 20 samples here, but this suggests that in general for this dataset and this proportion of a minority-majority split, we'd actually expect models to do better at telling apart the synthetic and confidential data for the majority group. To me, this makes sense because we have a larger training dataset. It suggests that my first method of synthesis, the one that was supposed to do "equally well" on the minority and majority groups, actually is still performing better on the majority group.

I'm now going to try to create a synthesis method that is closer to what we'd expect the difference of these metrics to be.

In the original synthesis method, visit sequence was ("SEX", "VETSTAT", "EMPSTAT", "EDUC", "HCOVANY", "RACE", "FTOTINC", "AGE"). This puts FTOTINC, which we know is correlated with poverty, near the end. I'm going to move it to the beginning, contradicting the 
```{r}
#define one-hot encoding function
dummy <- dummyVars(" ~ .", data=data_mi)

#perform one-hot encoding on data frame
final_df <- data.frame(predict(dummy, newdata=data_mi)) 
final_df <- final_df %>%
  dplyr::select(-c("SEX.Male", "HCOVANY.With.health.insurance.coverage", "EDUC.4..years.of.college", "EMPSTAT.Not.in.labor.force", "VETSTAT.Veteran"))

cor <- round(cor(final_df), 2)
data.frame(rownames(cor), data.frame(cor)[,"pov"])
```

```{r}
# synthesize categorical first, then continuous
visit_sequence = visit_sequence(c("FTOTINC", "EMPSTAT", "EDUC", "RACE", "SEX", "HCOVANY", "VETSTAT", "AGE"), start_data, type = "manual")
roadmap = roadmap(conf_data, start_data, visit_sequence)
recipe = construct_recipes(roadmap = roadmap)

tree_cl <- parsnip::decision_tree(cost_complexity = .0001) %>%
  set_mode(mode = "classification") %>%
  set_engine(engine = "rpart")
tree_reg <- parsnip::decision_tree(cost_complexity = .0001) %>%
  set_mode(mode = "regression") %>%
  set_engine(engine = "rpart")

synth_spec = synth_spec(roadmap,
                        synth_algorithms = list(tree_reg, tree_cl, tree_cl, tree_cl, tree_cl, tree_cl, tree_cl, tree_reg),
                        recipe,
                        predict_methods = sample_rpart)

# noise
noise <- noise(roadmap = roadmap,
               add_noise = FALSE,
               exclusions = 0)

# constraints
constraints <- constraints(roadmap = roadmap,
                           constraints = NULL,
                           max_z = 0)

replicates <- replicates(replicates = 1,
                         workers = 1,
                         summary_function = NULL)

# create a presynth object
presynth1 <- presynth(
  roadmap = roadmap,
  synth_spec = synth_spec,
  noise = noise, 
  constraints = constraints,
  replicates = replicates
)
synthesized_v2 = synthesize(presynth1, progress = TRUE)
```

Using this new synthesis to calculate the metrics
```{r}
eval_metrics <- function(synthesized_data, data_mi){
  synthesized_data_pov <- synthesized_data %>%
    filter(pov == 1)
  data_mi_pov <- data_mi %>%
    filter(pov == 1)
  synthesized_data_nopov <- synthesized_data %>%
    filter(pov == 0)
  data_mi_nopov <- data_mi %>%
    filter(pov == 0)

  # isolate to minority group
  d_synthesized_pov = discrimination(synthesized_data_pov, data_mi_pov)
  pov_tree = disc_tree(d_synthesized_pov)

  # isolate to majority group
  d_synthesized_nopov = discrimination(synthesized_data_nopov, data_mi_nopov)
  nopov_tree = disc_tree(d_synthesized_nopov)

  c(pov_tree, nopov_tree)
}
```

So reordering lowered our difference from .061 to .028, but it's still positive (our permutation tests were negative).
```{r}
auc_v2 <- eval_metrics(synthesized_v2$synthetic_data, data_mi)
auc_v2[1]-auc_v2[2]
```

I don't know how else to make these more similar to what (I think) we should be aiming for, which is a difference of about -.04.

However, this document shows we can tell when a synthesis is prioritizing majority groups, putting it in the context of whether/how much we expect the synthesis to do a better job for larger groups in general (here, I find we don't), and use variable reordering to lessen this bias. 

Here, I tell when the synthesis is prioritizing majority groups by isolating to the majority/minority group and then calculating the discriminant-based metrics on only the isolated majority/minority.


