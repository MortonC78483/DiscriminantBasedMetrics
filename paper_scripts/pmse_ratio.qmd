---
title: "pmse_ratio"
format:
  html:
    theme: default
---

This script will implement the two-model approach for the pMSE ratio discriminant-based metric.
For each element of the denominator of the pMSE ratio for the larger group, it will take a sample of size n (where n is the size of the smaller group) and then bootstrap this sample, taking a different initial sample each time. This will (according to the proof) create something with the same expected value under the null distribution as the minority group, allowing me to compare the 2-model approach's pMSE ratios.

I will code up the pMSE ratio function by group once I've gotten this version to work. For now, I'm doing it in a bit of a weird way.

```{r}
library(tidyverse)
library(ipumsr)
library(srvyr)
library(tidysynthesis)
library(parsnip)
library("data.table")
library(recipes)
library(devtools)
load_all("../../syntheval")
#library(syntheval)
library(dials)
library(tune)
library(gtools)
library(MASS)
library(caret)
library(gt)
library(ggpubr)
# source("../add_pmse_group.R")
source("../add_pmse_ratio_size.R")
source("../add_pmse_group.R")
source("../add_pmse_ratio_group.R")
# source("../add_discriminator_auc_group.R")
# source("../add_specks_group.R")
set.seed(1)
```

Load in data
```{r}
process_data <- function(df){
  df <- as.data.frame(unclass(df),stringsAsFactors=TRUE)
  df$pov <- as.factor(df$pov)
  df$BLACK <- as.factor(df$BLACK)
  order = c("pov", "EMPSTAT", "EDUC", "BLACK", "SEX", "HCOVANY", "VETSTAT", "FTOTINC", "AGE")
  df <- df %>%
    dplyr::select(order)
  df
}
```

```{r}
data_mi <- read_csv("../data/data_mi.csv")
data_mi <- process_data(data_mi)

synth_good <- read_csv("../data/good_synth_pov.csv")
synth_good <- process_data(synth_good)

synth_med <- read_csv("../data/med_synth_pov.csv")
synth_med <- process_data(synth_med)

synth_bad <- read_csv("../data/bad_synth_pov.csv")
synth_bad <- process_data(synth_bad)
```

```{r}
calc_disc <- function(discriminator){
  # Evaluate discriminant-based metrics on the data using tree-based model
  tree_mod <- decision_tree(cost_complexity = tune()) %>%
    set_mode(mode = "classification") %>%
    set_engine(engine = "rpart")
  
  rpart_rec <- recipe(.source_label ~ ., data = discriminator$combined_data)
  
  grid = grid_regular(cost_complexity(), levels= 10)
  
  # set up discriminator
  d <- discriminator %>%
    add_propensities_tuned(
      grid = grid, 
      recipe = rpart_rec,
      spec = tree_mod
    )
  d
}
```

Evaluate pMSE by group (numerator of pMSE ratio)
```{r}
disc_good = discrimination(synth_good, data_mi)
res_good = calc_disc(disc_good)

disc_med = discrimination(synth_med, data_mi)
res_med = calc_disc(disc_med)

disc_bad = discrimination(synth_bad, data_mi)
res_bad = calc_disc(disc_bad)
```

```{r}
times =3
split = T
prop = .75
add_pmse_ratio_group <- function(discrimination, split = TRUE, prop = 3 / 4, group = c(), times) {
  
  if (is.null(discrimination$pmse)) {
    
    stop("Error: discrimination must have a pmse. Use add_pmse() before add_pmse_ratio()")
    
  }
  
  calc_pmse <- function(propensities) {
    
    # calculate the expected propensity
    prop_synthetic <- propensities %>%
      dplyr::group_by(across(all_of(group))) %>%
      dplyr::summarize("prop_synthetic" = list(c(sum(.data$.source_label == "synthetic")/dplyr::n()))) %>%
      dplyr::pull(prop_synthetic)
    
    propensities_vec <- propensities %>%
      dplyr::group_by(across(all_of(group))) %>%
      dplyr::summarise(".pred_synthetic" = list(c(.pred_synthetic))) %>%
      dplyr::pull(.pred_synthetic)
    
    # function for pmse
    pmse_func <- function(propensities_vec, prop_synthetic){
      mean((propensities_vec - prop_synthetic)^2)
    }
    # calculate the observed pMSE
    pmse <- mapply(pmse_func, propensities_vec, prop_synthetic)
    
    return(pmse)
  }
  # function to sample 2x size of grouped data, by group, 
  # for the dataset with both confidential and synthetic data
  group_resample <- function(data){
    # split by grouping variables
    split_data = group_split(data %>% ungroup() %>% dplyr::group_by(across(all_of(group))))
    bootstrap_sample = list()
    for (elem in split_data){ # iterate through grouped dataset
      # in each group, sample twice as much data
      bootstrap_sample <- append(bootstrap_sample, list(dplyr::bind_cols(
        elem %>%
          dplyr::filter(.data$.source_label == "original") %>%
          dplyr::group_by(across(all_of(group))) %>%
          dplyr::slice_sample(n = nrow(elem), replace = TRUE) %>%
          dplyr::select(-".source_label"),
        elem %>%
          dplyr::select(".source_label"))
      ))
    }
    bootstrap_sample = bind_rows(bootstrap_sample)
  }
  
  # matrix instead of vector, where each entry is a simulation, containing a vector with groups
  pmse_null_overall <- c()
  pmse_null_training <- c()
  pmse_null_testing <- c()
  
  for (a in 1:times) {
    # bootstrap sample original observations to equal the size of the combined 
    # data, with a vector of one set of observations per grouping variable (?)
    # append the original labels so the proportions match
    bootstrap_sample <- group_resample(res_good$combined_data)

    if (split) {
      
      # make training/testing split 
      data_split <- rsample::initial_split(
        data = bootstrap_sample,
        prop = prop,
        strata = ".source_label" # NOTE: Should this also be stratified by group?
      )
      
      # fit the model from the pMSE on the bootstrap sample
      fitted_model <- parsnip::fit(
        res_good$discriminator, 
        data = rsample::training(data_split)
      )
      
      # calculate the propensities
      propensities_df <- dplyr::bind_cols(
        stats::predict(fitted_model, new_data = res_good$combined_data, type = "prob")[, ".pred_synthetic"],
        res_good$combined_data
      ) %>%
        dplyr::mutate(
          .sample = dplyr::if_else(
            dplyr::row_number() %in% data_split$in_id, 
            true = "training", 
            false = "testing"
          )
        )
      
      # calculate the pmse for each bootstrap
      pmse_null_overall <- append(pmse_null_overall, calc_pmse(propensities_df))
      pmse_null_training <- append(pmse_null_training, propensities_df %>%
        dplyr::filter(.data$.sample == "training") %>%
        calc_pmse())
      pmse_null_testing <- append(pmse_null_testing, propensities_df %>%
        dplyr::filter(.data$.sample == "testing") %>%
        calc_pmse())
      
    } else {
      
      # fit the model from the pMSE on the bootstrap sample
      fitted_model <- parsnip::fit(
        discrimination$discriminator, 
        data = bootstrap_sample
      )
      
      # calculate the propensities
      propensities_df <- dplyr::bind_cols(
        stats::predict(fitted_model, new_data = discrimination$combined_data, type = "prob")[, ".pred_synthetic"],
        discrimination$combined_data
      )
      
      # calculate the pmse for each bootstrap
      pmse_null_overall <- append(pmse_null_overall, calc_pmse(propensities_df))
      
    }
    
  }
  
  # find the mean of the bootstrapped pMSEs
  mean_null_pmse_overall <- colMeans(t(matrix(pmse_null_overall, ncol = times))) # each row is a new sample
  mean_null_pmse_training <- colMeans(t(matrix(pmse_null_training, ncol = times)))
  mean_null_pmse_testing <- colMeans(t(matrix(pmse_null_testing, ncol= times)))
  
  # calculate the ratio for the training/testing split or overall data
  if (all(c("training", "testing") %in% discrimination$pmse$.source)) {
    
    pmse <- dplyr::bind_cols(
      discrimination$pmse,
      tibble::tibble(.null_pmse = c(mean_null_pmse_training, mean_null_pmse_testing))
    ) %>%
      dplyr::mutate(.pmse_ratio = .data$.pmse / .data$.null_pmse)
    
  } else {
    
    pmse <- dplyr::bind_cols(
      discrimination$pmse,
      tibble::tibble(.null_pmse = mean_null_pmse_overall)
    ) %>%
      dplyr::mutate(.pmse_ratio = .data$.pmse / .data$.null_pmse)
    
  }
  
  discrimination$pmse <- pmse
  
  return(discrimination)
  
}
```

```{r}

data = res_good$combined_data
res_good <- res_good %>%
  add_pmse_group(c("pov")) %>%
  add_pmse_ratio_group(group = c("pov"), times = 2)
  
#resample = group_resample(res_good$combined_data)

```


```{r}
corrected_pmse_ratio <- function(synth, times, minority_size){
  # create no poverty sub-dataset
  conf_nopov <- data_mi %>%
    filter(pov == 0)
  synth_nopov <- synth %>%
    filter(pov == 0)
  disc_nopov <- discrimination(synth_nopov, conf_nopov)
  
  # fit model wtih pmse to our no poverty sub-dataset
  res_nopov <- calc_disc(disc_nopov)
  res_nopov = res
  # fit pmse ratio using size of minority group
  res_nopov <- res_nopov %>%
    add_pmse() %>%
    add_pmse_ratio_size(size = minority_size, times = times)
  
  # create poverty sub-dataset
  conf_pov <- data_mi %>%
    filter(pov == 1)
  synth_pov <- synth %>%
    filter(pov == 1)
  disc_pov <- discrimination(synth_pov, conf_pov)
  
  # fit model wtih pmse to our no poverty sub-dataset
  res_pov <- calc_disc(disc_pov)
  
  # fit pmse ratio using size of minority group
  res_pov <- res_pov %>%
    add_pmse() %>%
    add_pmse_ratio_size(size = minority_size, times = times)
  
  return(c(res_nopov$pmse$.pmse_ratio[2], res_pov$pmse$.pmse_ratio[2]))
}
```

```{r}
minority_size = sum(data_mi$pov == 1)
times = 10

pmse_ratio_good <- corrected_pmse_ratio(synth_good, times, minority_size)
pmse_ratio_med <- corrected_pmse_ratio(synth_med, times, minority_size)
pmse_ratio_bad <- corrected_pmse_ratio(synth_bad, times, minority_size)
```

Create a plot
```{r}
pmse_ratios = c(pmse_ratio_good, pmse_ratio_med, pmse_ratio_bad)
synth_type = c("good", "good", "med", "med", "bad", "bad")
pov_class = c("nopov", "pov", "nopov", "pov", "nopov", "pov")
synth_type = factor(synth_type, levels = c("bad", "med", "good"))

plot <- data.frame(pmse = pmse_ratios, synth_type = synth_type, pov_class = pov_class)
ggplot(plot, aes(x = synth_type, group = pov_class, y = pmse, color = pov_class))+
  geom_point()+
  ylab("pMSE Ratio")+
  xlab("Synthesis Type")+
  theme_classic()
```






